---
layout: post
title: Using Logistic Regression to Find Pulsars
excerpt: We analyse a dataset which contains information on the emission spectra of a number of “pulsar candidates”. Pulsars are a rare and interesting variety of neutron star which exhibit certain radio signals which allow them to be detectable from Earth. However, many similar signals come from radio interference and noise. Seeking to separate true pulsars from spurious candidates, we inspect the data and devise a number of GLMs (Generalized Linear Models) which allow us to predict pulsar presence based on signal characteristics. We select our best model and compare it to an alternative classification approach.
# categories: Statistics
tags: [Statistics, Astronomy]
---

The below post is adapted from a project I wrote for a class called “Generalized Linear Models and Data Analysis” in November 2018. The dataset is [HTRU2 from Robert J Lyon](https://archive.ics.uci.edu/ml/datasets/HTRU2) and some code is available [on this repository.](https://github.com/robsonedwards/pulsar-glm)

# Summary

We analyse a dataset which contains information on the emission spectra of a number of “pulsar candidates.” [Pulsars](https://en.wikipedia.org/wiki/Pulsar) are a rare and interesting variety of neutron star which exhibit certain radio signals which allow them to be detectable from Earth. However, many similar signals come from radio interference and noise. Seeking to separate true pulsars from spurious candidates, we inspect the data and devise a number of GLMs (Generalized Linear Models) which allow us to predict pulsar presence based on signal characteristics. We select our best model and compare it to an alternative classification approach.

# Introduction

[Pulsars](https://en.wikipedia.org/wiki/Pulsar) are a rare and interesting type of neutron star which rotate at a fast rate and transmit a periodic signal which can be detected by large radio telescopes on Earth. However, similar signals can be generated by noise or radio-frequency interference. Hence, classification of genuine pulsars from so-called “pulsar candidates” is an area of considerable scientific interest.<sup>[[3]](#sources-cited)</sup>

Other, relatively more recent machine-learning approaches have been applied to this problem with comparable results. We will return to this point later.

<!--TODO-->

Throughout this paper, we consider false negatives to be worse than false positives. The cost of a false negative is potentially missing out on a pulsar discovery, whereas the cost of a false positive is only the time it takes a researcher or grad student (quite disposable) to inspect the example and reject it by hand. This could change if the number of pulsar candidates we wish to inspect were to increase drastically.

<!--*TODO: move the above. improve or expand intro*-->

# Methods

We describe the dataset, followed by our methods for model formulation and selection as well as our motivations and justifications for the steps taken herein.

<!--TODO: *improve*-->

## Data

The data are the [HTRU2 dataset](https://archive.ics.uci.edu/ml/datasets/HTRU2), which describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey.<sup>[[3]](#sources-cited)</sup> 

The data consist of 17,898 observations of“pulsar candidates,” each represented by a single row. For each candidate we have eight continuous variables (V1 through V8) and one binary class variable V9, for a total of nine columns. V1 through V4 describe features of the integrated profile of the emission pattern from different pulsar candidates. V5 through V8 describe features of the candidate’s DM-SNR curves (dispersion measure and signal-to-noise ratio).<sup>[[3]](#sources-cited)</sup> Further details of these eight variables are unfortunately outside the scope of this item. The last variable, V9, is binary and labels the candidate’s class (authentic pulsar or spurious candidate). Fortuitously, every pulsar candidate is labelled, meaning this is a supervised learning problem. We use V1 through V8 to predict the class V9. Of the candidates, 1,639 are genuine pulsars and 16,259 are spurious. The first few rows of the dataset are shown below.  

#### Table 1

Data Sample

 V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | V9 | 
 ---|---|---|---|---|---|---|---|---|
 140.562 | 55.684 | -0.235 | -0.7 | 3.2 | 19.11 | 7.976 | 74.242 | 0 | 
 102.508 | 58.882 | 0.465 | -0.515 | 1.677 | 14.86 | 10.576 | 127.394 | 0 | 
 103.016 | 39.342 | 0.323 | 1.051 | 3.121 | 21.745 | 7.736 | 63.172 | 0 | 
 136.75 | 57.178 | -0.068 | -0.636 | 3.643 | 20.959 | 6.896 | 53.594 | 0 | 
 88.727 | 40.672 | 0.601 | 1.123 | 1.179 | 11.469 | 14.27 | 252.567 | 0 | 
 93.57 | 46.698 | 0.532 | 0.417 | 1.636 | 14.545 | 10.622 | 131.394 | 0 |

 We see that none of these first few candidates are genuine pulsars, because V9 is 0 for each. 

We plot relationships between the eight explanatory variables in [Figure 1](#figure-1). It can be seen, roughly speaking, that the pulsars (cyan) and non-pulsars (orange) are well-separated on many of these variables. Hence, we imagine that we could address our prediction problem with a binomial GLM.

#### Figure 1

**Scatter Plots (click to view larger)**

[![Figure 1: Exploratory Data Analysis](/assets/scattermatrix.svg)](/assets/scattermatrix.svg)

There are at least two “problems” with the data. First, the explanatory variables are not particularly interpretable, at least with my present level of understanding of signal processing and of astronomy. This is not a massive obstacle for us as we are more interested in prediction than inference, so our final model does not need to be highly interpretable. The second problem is multicollinearity. It can be seen from [Figure 1](#figure-1) that, in particular, there are high correlations between V3 and V4, and between V7 and V8. In fact, the correlations (Pearson product moment coefficient) are 0.95 and 0.92, respectively. We will address this issue [later](#table-2). All other absolute correlations are below 0.9.

## Model Formulation

We consider how to fit a generalized linear model to the data. First, we fit a model with all eight explanatory variables and no interactions or higher-order terms. This model assumes that pulsar candidates are distributed according to a Bernoulli distribution where *p*, the probability of a given candidate being a genuine pulsar, is logit-linear to a weighted sum of V1-V8.

This initial model, henceforth Model 1, works reasonably well. Model 1 has AIC 2633.8 ([Akaike information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion), which is a function of model complexity and likelihood, lower better). We construct a natural classifier from this model, where we classify all candidates with predicted *p̂* > 0.5 as pulsars and with *p̂* ≤ 0.5 as non-pulsars. Now we can quantify the quality of that classifier. We refer to the *accuracy* of a classifier as the overall proportion of correctly classified candidates, the *specificity* as the proportion of non-pulsars which it correctly rejects, and the *sensitivity* as the proportion of genuine pulsars which it correctly accepts. Then, Model 1 gives rise to “Classifier 1,” which has accuracy 0.980, sensitivity 0.828, and specificity 0.995. This is pretty good for a first start.

Selecting a cutoff other than 0.5 can significantly affect the quality of the derived classifier. We plot the accuracy, sensitivity and specificity of the space of all classifiers derived from Model 1 with different cutoffs between 0 and 1 in Figure 2.

#### Figure 2

**Classifiers derived from Model 1**

[![Figure 2: Classifiers from Model 1](/assets/cutoffs.svg)](/assets/cutoffs.svg)

There are three main problems with our first model. First, Classifier 1 is too conservative in classifying pulsar candidates. We note that we can improve this by reducing the classification cutoff (which was *p̂* > 0.5 above). If we select the cutoff to maximize accuracy, we find that a cutoff *p̂* > 0.364 gives us an improved classifier “Classifier 1B” with accuracy 0.981, sensitivity 0.853, and specificity 0.993. Recall Classifier 1 had 0.980, 0.828, 0.995 respectively. We consider this to be a better classifier because of the improved accuracy and significantly improved sensitivity. We are particularly interested in sensitivity, because our goal is to find pulsars, and so the cost of missing a true pulsar is “worse” than incorrectly accepting a non-pulsar. Thus, roughly speaking, gaining 0.025 sensitivity is “worth” losing 0.002 specificity.

Note that it would not be reasonable to select the cutoff by maximizing sensitivity, as this would simply give us a cutoff of 0 with sensitivity 1, and accept every non-pulsar. It would also be absurd to maximize specificity, as this would give us a cutoff of 1 and reject every pulsar. Hence, we maximize accuracy. There exist other reasonable metrics of classifier quality but accuracy is sufficient for our purposes.

The second problem is that the effects of V7 and V8 are not significant in the model. The p-value for the effect of V7 is 0.58 and for V8, 0.12. As we will see momentarily, this can resolved by removing either from the model, which reduces the overall multicollinearity of the predictors and allows the remaining effect to become significant.<sup>[[A]](#notes)</sup>

The third problem is the collinearity issue identified in [Data](#data). V3 and V4 are highly correlated (0.95) and V7 and V8 are highly correlated (0.92). We calculate the VIFs (variance inflation factors) for Model 1 and present them in Table 2. The VIF for a single explanatory variable *X* is equal to 1/(1-R<sup>2</sup>), where R is the correlation coefficient obtained from a simple linear regression onto *X* by all other terms in the model. Often 5 or 10 are used as reasonable cutoffs for VIF, as these imply that the appropriate standard errors are overinflated by only 5 or 10, respectively. V3, V7, and V8 have large VIFs above the cutoff, so we shall now fit and analyse three more models, one each without V3, V7, and V8. Each of these models has all significant effects. The model without V8 has AIC 2634.2. The model without V7 has AIC 2632.1.

#### Table 2

**Variance Inflation Factors**

Model | V1 | V2 | V3 | V4 | V5 | V6 | V7 | V8 | AIC
------|----|----|----|----|----|----|----|----|---
1     |4.21|1.73|12.35 | 6.98 | 3.96 | 9.61 | 37.03 | 15.88 | 2633.8
2     |4.21|1.72|12.31 | 6.98 | 3.49 | 4.42 | — | 1.82 | 2632.1
3     |3.96|1.34|4.08 | — | 3.62 | 4.69 | — | 1.81 | 2733.3

The model without V3, unfortunately, has numerical issues which prevent the algorithm we used from finding a good fit.<sup>[[A]](#notes)</sup> Eleven of the pulsars are fitted to linear predictor values greater than 30. These give fitted values which are numerically equivalent to 1 (as 1 / [1 + e<sup>-30</sup>] is extremely close to 1). This model has AIC 3136.3. Also, the natural classifier it suggests has accuracy 0.975, sensitivity 0.774, and specificity 0.995. This model is so much worse than what we’ve been working with so far that we omit it.

I am not sure how to remedy the numerical issues. I know that we do not have perfect separation, which can give rise to a similar issue. I know this because there is not perfect separation in any of V1 through V8, and because the greatest fitted prediction value among non-pulsars is *p̂* = 0.9999993, and the least among pulsars is *p̂* = 0.00104. So there is some overlap in the classes on the predictor scale under this model.

One solution to the numerical issues is to simply remove V4 from the model rather than V3. This succeeds in reducing the VIFs but increases the AIC to 2733.3. This is much higher than what we’ve had so far. This model is listed in Table 1 as Model 3.

We decide that the model without V7, henceforth “Model 2”, is the best yet. Conducting a stepwise selection by AIC, starting with Model 1, also selects Model 2.

We can consider larger models.

First, we consider “Model 4”, which is the model with all eight explanatory variables and also all 28 interactions between them. After fitting, this model has an AIC of 2466.8. Across this model’s 36 inputs, it has VIFs up to 9,217. We also fit “Model 5”, based on a stepwise selection from this model. Model 5 throws out fifteen interaction terms and reduces the AIC to 2445 and the maximum VIF to 1,241. Note that because we’ve now included interactions in the model, multicollinearity is less of a problem than it was before. It is not really surprising that e.g. V1 and V1*V2 are highly correlated. 

As shown in [Table 3](table-3), the *p̂* > 0.5 classifiers dervied from Models 4 and 5, are both almost exactly as accurate and specific as Model 2, and are slightly more sensitive. 

## Model Selection

It is known that Akaike Information Criterion is more appropriate than deviance and other likelihood-ratio statistics for model comparison in the interest of prediction, rather than inference. This is why we have been using AIC to compare models so far. VIFs, as we have also been using, are a measure of excess dispersion in a model. We also have three more model metrics<sup>[[B]](#notes)</sup>, which are accuracy, specificity and sensitivity as introduced in [Model Formulation](#model-formulation). For each of these last three, we give the values both for a simple classification rule with cutoff *p̂* > 0.5, and also for an accuracy-maximizing cutoff. In Table 2, we present all named models so far and compare their performance on these eight measures (and present the optimal cutoff). The most important metrics to us are AIC, accuracy, and sensitivity, perhaps in that order.

#### Table 3

 Model Comparison

| | | |Cutoff 0.5|"|"| |Best Cutoff|"|"|
--------|-----|---------|-------|-------|-------|-------------|-------|-------|------
Model   | AIC | Max VIF | Accu. | Spec. | Sens. | Best Cutoff | Accu. | Spec. | Sens. 
1 | 2633.8 | 37.03  | 0.981 | 0.995 | 0.828 | 0.364 | 0.981 | 0.993 | 0.853
2 | 2632.1 | 12.31  | 0.981 | 0.995 | 0.827 | 0.342 | 0.981 | 0.993 | 0.858
3 | 2733.3 | 4.69   | 0.980 | 0.995 | 0.815 | 0.265 | 0.980 | 0.992 | 0.860
4 | 2466.8 | 9217.7 | 0.981 | 0.994 | 0.843 | 0.373 | 0.981 | 0.992 | 0.868
5 | 2445.5 | 1241.6 | 0.981 | 0.994 | 0.844 | 0.432 | 0.981 | 0.993 | 0.858
           
Accuracy for these models seems to top out around 98.1%. Of the models, Model 4 allows us to construct the most sensitive classifier. Model 2 does not permit such a sensitive classifier but it is simpler. Models 1, 3, and 5, and all the other unnamed models we mentioned above are each “strictly worse” than at least one other model and so should never be used.

We plot the accuracy, specificity and sensitivity of all classifiers derived from Model 4 in Figure 3. The commensurate curves from Model 1 are plotted in grey dashed lines on the same figure to show the improvement. It can be seen that, if we are willing to sacrifice some accuracy for sensitivity, we could use Model 4 to create a classifier with say cutoff 0.048, and accuracy, sensitivity, and specificity all around 0.938.

#### Figure 3

[![Figure 3: Classifiers from Model 4](/assets/cutoffs4.svg)](/assets/cutoffs4.svg)

# Results and Conclusion

We have managed to formulate GLMs which, in the best case, can classify pulsar candidates correctly 98.1% of the time, with 86.8% of genuine pulsars identified correctly, or can identify 93.8% of candidates and of true pulsars correctly. This is roughly comparable to the results of some more recent machine-learning approaches to pulsar classification. In 2010, Eatough et al. [[2]](#sources-cited) suggested a neural network approach which correctly classified 92% of pulsars, albeit in a larger dataset. They also penalized false positives more than false negatives, which is the opposite of how we handled the problem.

Model 4 is not particularly interpretable for inference, given that the predictors themselves are rather uninterpretable and the model has twenty-three different parameters (coefficients for the eight parameters and fifteen interactions). However, it is still more interpretable than the above neural network. 

There are a number of interesting avenues for further study. One such avenue would certainly be working out the numerical issues in [Model Formulation](#model-formulation). Also, perhaps there exists some transformation of variables (e.g. [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)) that would reduce collinearity and allow for a model with low dispersion without losing predictive power or fitting such extreme points on the linear scale that numerical issues arise. I would also be interested to see whether we could fit a GLM with greater accuracy and sensitivity, perhaps through model formulations with more interactions and nonlinear terms.

# Sources Cited

[1] R Core Team (2018). *R: A language and environment for statistical computing*. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.

[2] R. P. Eatough et al., *‘Selection of radio pulsar candidates using artificial neural networks’*, Monthly Notices of the Royal Astronomical Society, vol. 407, no. 4, pp. 2443-2450, 2010.

[3] R. J. Lyon, B. W. Stappers, S. Cooper, J. M. Brooke, J. D. Knowles, Fifty Years of Pulsar Candidate Selection: From simple filters to a new principled real-time classification approach, Monthly Notices of the Royal Astronomical Society 459 (1), 1104-1123, DOI: 10.1093/mnras/stw656

[4] R. J. Lyon, HTRU2, DOI: 10.6084/m9.figshare.3080389.v1.

# Notes

[A] This could be a whole paper (and likely already is). 

[B] Which are not metrics in the analytic sense. 

